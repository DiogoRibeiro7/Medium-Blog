{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d81f5e9",
   "metadata": {},
   "source": [
    "# Pre Processing Data for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe11cf",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8e0cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import boxcox\n",
    "from scipy.stats import yeojohnson\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe33812",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53909e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmart_data = pd.read_csv('bigmartsales.csv')\n",
    "bigmart_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbc4824",
   "metadata": {},
   "source": [
    "## 1(a) Exploratory Data Analysis\n",
    "\n",
    "First, we'll start with a general overview of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6020f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bigmart_data.describe(include='all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa78a0ab",
   "metadata": {},
   "source": [
    "### Here are the descriptive statistics for the BigMart dataset:\n",
    "\n",
    "**Item_Identifier:** There are 1,559 unique item identifiers, with the most frequent one being \"FDW13\".\n",
    "\n",
    "**Item_Weight:** The weights of items range from 0 to 21.35, with an average weight of approximately 11.68.\n",
    "\n",
    "**Item_Fat_Content:** This categorical feature has 5 unique values, with \"Low Fat\" being the most frequent.\n",
    "\n",
    "**Item_Visibility:** The average visibility is 0.066, and it ranges from 0 to 0.328.\n",
    "\n",
    "**Item_Type:** There are 16 unique item types, with \"Fruits and Vegetables\" being the most frequent.\n",
    "\n",
    "**Item_MRP:** The maximum retail price (MRP) of items ranges from 31.30 to 266.90, with an average of about 141.\n",
    "\n",
    "**Outlet_Identifier:** There are 10 unique outlets, with \"OUT027\" being the most frequent.\n",
    "\n",
    "**Outlet_Establishment_Year:** The outlets were established between 1985 and 2009.\n",
    "\n",
    "**Outlet_Size:** This categorical feature has 3 unique sizes, with \"Medium\" being the most frequent. However, there are missing values in this column.\n",
    "\n",
    "**Outlet_Location_Type:** There seems to be a discrepancy as there are 8 unique location types, which might be due to inconsistent labeling.\n",
    "\n",
    "**Outlet_Type:** There are 4 unique outlet types, with \"Supermarket Type1\" being the most frequent.\n",
    "\n",
    "**Item_Outlet_Sales:** The sales range from 33.29 to 13,086.96, with an average of about 2,181.29.\n",
    "\n",
    "**Profit:** The profit percentage ranges from 0.1% to 24%, with an average of around 13.41%.\n",
    "\n",
    "Next, let's visualize the distribution of numerical and categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b65f05",
   "metadata": {},
   "source": [
    "### EDA of numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebaba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_features = bigmart_data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Histogram for all numeric features\n",
    "numeric_features.hist(figsize=(15, 10), bins=50)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecea1719",
   "metadata": {},
   "source": [
    "The histograms above show the distribution of the numerical features in the dataset. Here's a brief summary:\n",
    "\n",
    "**Item_MRP:** The maximum retail price seems to have multiple peaks, indicating different pricing categories or clusters.\n",
    "    \n",
    "**Item_Outlet_Sales:** The sales distribution is right-skewed, with a majority of items having lower sales.\n",
    "    \n",
    "**Item_Visibility:** It's also right-skewed, indicating that many items have low visibility in the store.\n",
    "    \n",
    "**Item_Weight:** Appears somewhat uniformly distributed, with slight variations.\n",
    "    \n",
    "**Outlet_Establishment_Year:** The distribution shows peaks corresponding to specific years, indicating when most outlets were established.\n",
    "    \n",
    "**Profit:** The profit percentage appears left-skewed with most items having a profit percentage between 10% and 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e810bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting numeric features excluding the target 'Item_Outlet_Sales'\n",
    "features = bigmart_data.select_dtypes(include=[np.number]).drop(columns=['Item_Outlet_Sales'])\n",
    "\n",
    "# Plotting scatter plots for each numeric feature against 'Item_Outlet_Sales'\n",
    "for feature in features.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(bigmart_data[feature], bigmart_data['Item_Outlet_Sales'], alpha=0.5)\n",
    "    plt.title(f'Relationship between {feature} and Item_Outlet_Sales')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Item_Outlet_Sales')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae77399",
   "metadata": {},
   "source": [
    "### EDA of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee9dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categorical_features = bigmart_data.select_dtypes(exclude=[np.number])\n",
    "\n",
    "# Countplot for all categorical features\n",
    "for feature in categorical_features.columns:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.countplot(data=bigmart_data, x=feature)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting categorical features\n",
    "categorical_features = bigmart_data.select_dtypes(exclude=[np.number])\n",
    "\n",
    "# Plotting bar charts for each categorical feature\n",
    "for feature in categorical_features.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bigmart_data[feature].value_counts().plot(kind='bar')\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel(feature)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13597180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting categorical features\n",
    "categorical_features = bigmart_data.select_dtypes(exclude=[np.number])\n",
    "\n",
    "# Plotting box plots for each categorical feature against 'Item_Outlet_Sales'\n",
    "for feature in categorical_features.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=bigmart_data, x=feature, y='Item_Outlet_Sales')\n",
    "    plt.title(f'Impact of {feature} on Item_Outlet_Sales')\n",
    "    plt.ylabel('Item_Outlet_Sales')\n",
    "    plt.xlabel(feature)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54071c06",
   "metadata": {},
   "source": [
    "## 2. Deal with Missings\n",
    "\n",
    "Identify columns with missing values and decide on strategies to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2836c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = bigmart_data.isnull().sum()\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7017a",
   "metadata": {},
   "source": [
    "### For imputation, we have several strategies:\n",
    "\n",
    "**Item_Weight:** We can replace missing values with the mean or median of the Item_Weight column.\n",
    "\n",
    "**Outlet_Size:** Since it's a categorical variable, we can replace missing values with the mode (most frequent category) of the Outlet_Size column.\n",
    "\n",
    "**Outlet_Location_Type:** Similarly, being a categorical variable, we can use the mode for imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327f5132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation for 'Item_Weight' using the mean\n",
    "bigmart_data['Item_Weight'].fillna(bigmart_data['Item_Weight'].mean(), inplace=True)\n",
    "\n",
    "# Imputation for 'Outlet_Size' using the mode\n",
    "bigmart_data['Outlet_Size'].fillna(bigmart_data['Outlet_Size'].mode()[0], inplace=True)\n",
    "\n",
    "# Imputation for 'Outlet_Location_Type' using the mode\n",
    "bigmart_data['Outlet_Location_Type'].fillna(bigmart_data['Outlet_Location_Type'].mode()[0], inplace=True)\n",
    "\n",
    "# Verify if missing values are handled\n",
    "missing_values_after = bigmart_data.isnull().sum()\n",
    "missing_columns_after = missing_values_after[missing_values_after > 0]\n",
    "print(missing_columns_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c58e15",
   "metadata": {},
   "source": [
    "# Duplicates and Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b983ad",
   "metadata": {},
   "source": [
    "### Duplicates\n",
    "Identify and count duplicate rows.\n",
    "\n",
    "Remove duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd240d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and count duplicate rows\n",
    "duplicate_rows = bigmart_data.duplicated().sum()\n",
    "print(f'Number of duplicate rows: {duplicate_rows}')\n",
    "\n",
    "# Remove duplicate rows\n",
    "bigmart_data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Verify removal of duplicates\n",
    "duplicate_rows_after = bigmart_data.duplicated().sum()\n",
    "print(f'Number of duplicate rows after removal: {duplicate_rows_after}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cb4df5",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Visualize the distribution of numerical features using box plots to identify potential outliers.\n",
    "\n",
    "Decide on a strategy to handle outliers (e.g., removal, capping, or transformation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5b44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting numeric features from the dataset\n",
    "numeric_features = bigmart_data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Plotting box plots for each numeric feature to visualize potential outliers\n",
    "for feature in numeric_features.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(bigmart_data[feature])\n",
    "    plt.title(f'Box plot of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be197ab0",
   "metadata": {},
   "source": [
    "After visualizing the outliers, the next steps typically involve deciding how to handle them. Common strategies include:\n",
    "\n",
    "**Removal:** This involves simply removing the outlier data points. It's a direct method but can lead to a loss of data.\n",
    "\n",
    "**Capping:** Outliers are set to a particular maximum or minimum value. This can be set based on domain knowledge or specific percentiles.\n",
    "\n",
    "**Transformation:** Applying transformations like log, square root, or others to compress the extreme values.\n",
    "**Binning:** Convert continuous variables into categorical counterparts by creating intervals.\n",
    "\n",
    "To proceed, we first need to define what we consider an \"outlier.\" A common method is to use the IQR (Interquartile Range) rule:\n",
    "\n",
    "Any value below Q1−1.5×IQR or above Q3+1.5×IQR is considered an outlier, where Q1 and Q3 are the first and third quartiles, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5586ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect outliers based on IQR\n",
    "def detect_outliers(data):\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR)))\n",
    "    return outliers\n",
    "\n",
    "outliers_detected = numeric_features.apply(detect_outliers)\n",
    "outliers_count = outliers_detected.sum()\n",
    "print(outliers_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbf0424",
   "metadata": {},
   "source": [
    "Transformations can be an effective way to manage outliers, especially for data that is skewed. Common transformations include:\n",
    "\n",
    "**Log Transformation:** Useful for right-skewed data.\n",
    "    \n",
    "**Square Root Transformation:** Helps to reduce the impact of extreme values.\n",
    "    \n",
    "**Box-Cox Transformation:** A family of power transformations that can stabilize variance and make the data more normal in distribution.\n",
    "\n",
    "Given the nature of sales data and the usual presence of zero values in features like Item_Visibility, the log transformation might be tricky due to the logarithm of zero being undefined. Therefore, we can use a slight adjustment by adding a small constant to ensure there are no zero values.\n",
    "\n",
    "Let's start by visualizing the distribution of these features using histograms to determine their skewness and then decide on an appropriate transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7196b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_transform = ['Item_Visibility', 'Item_Outlet_Sales', 'Profit']\n",
    "\n",
    "# Plotting histograms for the specified features\n",
    "for feature in features_to_transform:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(bigmart_data[feature], kde=True)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b4f1f3",
   "metadata": {},
   "source": [
    "The Box-Cox transformation is indeed a powerful method to handle skewed data. It's defined as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y_{\\text{transformed}} =\n",
    "\\begin{cases}\n",
    "\\frac{y^\\lambda - 1}{\\lambda}, & \\text{if } \\lambda \\neq 0 \\\\\n",
    "\\ln(y), & \\text{if } \\lambda = 0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "Y is the data you want to transform.\n",
    "\n",
    "λ is the transformation parameter.\n",
    "\n",
    "The Box-Cox transformation requires all data to be positive. So before applying it, we'll ensure the data is positive. Also, we'll use the scipy library's boxcox function, which automatically finds the lambda that maximizes the normality of the data.\n",
    "\n",
    "Here's the Python code to apply the Box-Cox transformation to the specified features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32285365",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_transform = ['Item_Visibility', 'Item_Outlet_Sales']\n",
    "\n",
    "# Ensure data is positive by adding a small constant to 'Item_Visibility'\n",
    "bigmart_data['Item_Visibility'] = bigmart_data['Item_Visibility'] + 0.01\n",
    "\n",
    "# Applying Box-Cox transformation\n",
    "for feature in features_to_transform:\n",
    "    bigmart_data[feature], _ = boxcox(bigmart_data[feature])\n",
    "\n",
    "\n",
    "for feature in features_to_transform:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(bigmart_data[feature], kde=True)\n",
    "    plt.title(f'Distribution of {feature} after Box-Cox Transformation')\n",
    "    plt.xlabel(feature)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a73195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all values are positive by adding a small constant to 'Profit'\n",
    "bigmart_data['Profit'] = bigmart_data['Profit'] + abs(bigmart_data['Profit'].min()) + 0.01\n",
    "\n",
    "# Applying the log transformation\n",
    "bigmart_data['Profit_log_transformed'] = np.log(bigmart_data['Profit'])\n",
    "\n",
    "# Visualizing the transformed data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(bigmart_data['Profit_log_transformed'], kde=True)\n",
    "plt.title('Distribution of Profit after Log Transformation')\n",
    "plt.xlabel('Profit (Log Transformed)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369d3105",
   "metadata": {},
   "source": [
    "The log transformation may not always produce a perfectly normal distribution, especially if the original data is highly skewed or has a complex distribution.\n",
    "\n",
    "Another transformation you might consider is the inverse transformation, especially when the data is skewed to the right. For data skewed to the left, a square or cube transformation might be effective.\n",
    "\n",
    "Let's try the inverse transformation on the Profit feature:\n",
    "\n",
    "Here's the Python code to apply the inverse transformation to the Profit feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb63bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the inverse transformation to the 'Profit' feature\n",
    "bigmart_data['Profit_inverse_transformed'] = 1 / bigmart_data['Profit']\n",
    "\n",
    "# Visualizing the transformed data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(bigmart_data['Profit_inverse_transformed'], kde=True)\n",
    "plt.title('Distribution of Profit after Inverse Transformation')\n",
    "plt.xlabel('Profit (Inverse Transformed)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78458b5",
   "metadata": {},
   "source": [
    "If the Profit feature has a complicated distribution, common transformations like log and inverse might not produce a perfectly normal distribution. Let's explore a few more alternatives:\n",
    "\n",
    "**Square Transformation:** For left-skewed data.\n",
    "\n",
    "**Sqrt Transformation:** For right-skewed data.\n",
    "\n",
    "**Yeo-Johnson Transformation:** A more flexible method that can handle zeros in the data.\n",
    "\n",
    "**Quantile Transformer:** Maps the data to a uniform distribution.\n",
    "\n",
    "Given the right skew, we can try the Sqrt transformation and the Yeo-Johnson transformation. The Quantile Transformer is also worth considering, as it makes fewer assumptions about the functional form of the transformation.\n",
    "\n",
    "Let's start with the Sqrt transformation on the Profit feature:\n",
    "\n",
    "Here's the Python code to apply the Sqrt transformation to the Profit feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc3b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the square root transformation to the 'Profit' feature\n",
    "bigmart_data['Profit_sqrt_transformed'] = np.sqrt(bigmart_data['Profit'])\n",
    "\n",
    "# Visualizing the transformed data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(bigmart_data['Profit_sqrt_transformed'], kde=True)\n",
    "plt.title('Distribution of Profit after Square Root Transformation')\n",
    "plt.xlabel('Profit (Square Root Transformed)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc25ec3",
   "metadata": {},
   "source": [
    "The Yeo-Johnson transformation is a flexible method that can handle both positive and negative values in the data, making it a generalization of the Box-Cox transformation. It's particularly useful when data has zeros or negative values.\n",
    "\n",
    "Let's apply the Yeo-Johnson transformation to the Profit feature.\n",
    "\n",
    "Here's the Python code to apply the Yeo-Johnson transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d0c141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the Yeo-Johnson transformation to the 'Profit' feature\n",
    "bigmart_data['Profit_yeojohnson_transformed'], lambda_value = yeojohnson(bigmart_data['Profit'])\n",
    "\n",
    "# Visualizing the transformed data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(bigmart_data['Profit_yeojohnson_transformed'], kde=True)\n",
    "plt.title('Distribution of Profit after Yeo-Johnson Transformation')\n",
    "plt.xlabel('Profit (Yeo-Johnson Transformed)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6947ab3b",
   "metadata": {},
   "source": [
    "The Quantile Transformer is a robust method that can map data to a uniform distribution (or a Gaussian distribution) by ranking the data. This can be particularly useful for dealing with features that have heavy tails or features that are on different scales.\n",
    "\n",
    "The QuantileTransformer class in the sklearn.preprocessing module can be used for this transformation.\n",
    "\n",
    "Here's the Python code to apply the Quantile Transformer to the Profit feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a11ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the Quantile Transformer to the 'Profit' feature\n",
    "transformer = QuantileTransformer(output_distribution='uniform', random_state=0)\n",
    "bigmart_data['Profit_quantile_transformed'] = transformer.fit_transform(bigmart_data[['Profit']])\n",
    "\n",
    "# Visualizing the transformed data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(bigmart_data['Profit_quantile_transformed'], kde=True)\n",
    "plt.title('Distribution of Profit after Quantile Transformation')\n",
    "plt.xlabel('Profit (Quantile Transformed)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32543902",
   "metadata": {},
   "source": [
    "# Feature Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5141ebe1",
   "metadata": {},
   "source": [
    "Feature encoding is essential when dealing with categorical variables, especially for machine learning algorithms that expect numerical input values.\n",
    "\n",
    "There are several methods for encoding categorical variables. Some of the most common ones include:\n",
    "\n",
    "**One-Hot Encoding:** This method creates a binary column for each category in the original column. It's suitable for nominal data where no ordinal relationship exists.\n",
    "    \n",
    "**Label Encoding:** This method assigns a unique integer to each category. It's more suitable for ordinal data where an order between categories exists, but can be used for nominal data with tree-based algorithms.\n",
    "    \n",
    "**Ordinal Encoding:** Similar to label encoding but used explicitly for ordinal data where an order between categories exists.\n",
    "    \n",
    "**Frequency or Count Encoding:** Replace categories by the count of the occurrences or the frequency (proportion).\n",
    "\n",
    "**Target Encoding (Mean Encoding):** Replace categories by the average target value for that category. Care must be taken to avoid data leakage.\n",
    "\n",
    "For the Bigmart dataset, there are several categorical variables. We'll start by identifying them and then decide on the appropriate encoding technique for each. Let's first list out the categorical features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941dfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying categorical features in the dataset\n",
    "categorical_features = bigmart_data.select_dtypes(include=['object']).columns\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa5d23",
   "metadata": {},
   "source": [
    "Let's examine each of the categorical features in the dataset and provide recommendations based on their nature:\n",
    "\n",
    "**Item_Identifier:** This is an identification code for the product. As it likely has a large number of unique values, one-hot encoding might significantly increase the dimensionality of the dataset. A better approach might be to drop this feature unless it's essential for specific analyses or to use feature engineering to derive useful information from it.\n",
    "\n",
    "**Item_Fat_Content:** This is nominal data representing the fat content of the item. One-hot encoding would be appropriate here. Additionally, this column might have inconsistent labeling (e.g., \"Low Fat\", \"LF\" both indicating low fat), so it would be good to standardize the labels first.\n",
    "\n",
    "**Item_Type:** This is the type or category of the item, which is nominal data. One-hot encoding is recommended.\n",
    "\n",
    "**Outlet_Identifier:** Like Item_Identifier, this is an identification code for the outlet. If there's a business reason to retain this feature, one-hot encoding might be used. Alternatively, it can be dropped or used to derive other features.\n",
    "\n",
    "**Outlet_Establishment_Year:** This feature is ordinal since there's a clear order (older or newer year). While it's already in a numeric format, it might be more useful to convert this to the age of the outlet (e.g., \"2023 - Outlet_Establishment_Year\") to give a clearer picture of how old each outlet is.\n",
    "\n",
    "**Outlet_Size:** This represents the size of the outlet and is ordinal data (e.g., Small < Medium < Large). Ordinal encoding can be used here.\n",
    "\n",
    "**Outlet_Location_Type:** This is ordinal data representing the type of city in which the store is located. Ordinal encoding is suitable.\n",
    "\n",
    "**Outlet_Type:** This is the type of outlet, which is nominal data. One-hot encoding is recommended.\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "**Item_Identifier:** Drop or feature engineering.\n",
    "\n",
    "**Item_Fat_Content:** One-hot encoding (after standardizing labels).\n",
    "\n",
    "**Item_Type:** One-hot encoding.\n",
    "\n",
    "**Outlet_Identifier:** Drop or one-hot encoding.\n",
    "\n",
    "**Outlet_Establishment_Year:** Convert to outlet age.\n",
    "\n",
    "**Outlet_Size:** Ordinal encoding.\n",
    "\n",
    "**Outlet_Location_Type:** Ordinal encoding.\n",
    "\n",
    "**Outlet_Type:** One-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda4e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping 'Item_Identifier' and 'Outlet_Identifier'\n",
    "bigmart_data.drop(['Item_Identifier', 'Outlet_Identifier'], axis=1, inplace=True)\n",
    "\n",
    "# Standardizing labels in 'Item_Fat_Content'\n",
    "fat_content_mapping = {\n",
    "    'Low Fat': 'Low Fat', \n",
    "    'Regular': 'Regular', \n",
    "    'LF': 'Low Fat', \n",
    "    'reg': 'Regular', \n",
    "    'low fat': 'Low Fat'\n",
    "}\n",
    "bigmart_data['Item_Fat_Content'] = bigmart_data['Item_Fat_Content'].map(fat_content_mapping)\n",
    "\n",
    "# One-hot encoding for 'Item_Fat_Content', 'Item_Type', and 'Outlet_Type'\n",
    "one_hot_encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "one_hot_encoded_columns = ['Item_Fat_Content', 'Item_Type', 'Outlet_Type']\n",
    "one_hot_encoded_data = one_hot_encoder.fit_transform(bigmart_data[one_hot_encoded_columns])\n",
    "one_hot_encoded_df = pd.DataFrame(one_hot_encoded_data, columns=one_hot_encoder.get_feature_names_out(one_hot_encoded_columns))\n",
    "bigmart_data = pd.concat([bigmart_data, one_hot_encoded_df], axis=1)\n",
    "bigmart_data.drop(one_hot_encoded_columns, axis=1, inplace=True)\n",
    "\n",
    "# Convert 'Outlet_Establishment_Year' to outlet age\n",
    "current_year = 2023\n",
    "bigmart_data['Outlet_Age'] = current_year - bigmart_data['Outlet_Establishment_Year']\n",
    "bigmart_data.drop('Outlet_Establishment_Year', axis=1, inplace=True)\n",
    "\n",
    "# Ordinal encoding for 'Outlet_Size' and 'Outlet_Location_Type'\n",
    "ordinal_encodings = {\n",
    "    'Outlet_Size': {'Small': 1, 'Medium': 2, 'High': 3},\n",
    "    'Outlet_Location_Type': {'Tier 3': 3, 'Tier 2': 2, 'Tier 1': 1}\n",
    "}\n",
    "bigmart_data.replace(ordinal_encodings, inplace=True)\n",
    "\n",
    "bigmart_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3543b7ae",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee08eb23",
   "metadata": {},
   "source": [
    "Feature scaling ensures that all features contribute equally to the computation of distances. There are two common methods:\n",
    "\n",
    "**Standardization (Z-score normalization):** This method scales the features such that they have the properties of a standard normal distribution with a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "\\end{equation}\n",
    "$$\n",
    " \n",
    "Where:\n",
    "x = original feature value\n",
    "μ = mean of the feature\n",
    "σ = standard deviation of the feature\n",
    "\n",
    "**Min-Max Scaling:** This method scales and translates each feature individually such that it is in the given range on the training set, typically between zero and one.\n",
    "\n",
    "Formula:\n",
    "$$\n",
    "\\begin{equation}\n",
    "x_{\\text{scaled}} = \\frac{x - \\min(X)}{\\max(X) - \\min(X)}\n",
    "\\end{equation}\n",
    "$$\n",
    " \n",
    "Most machine learning algorithms perform better when numerical input variables are scaled to a standard range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d48399",
   "metadata": {},
   "source": [
    "Let's proceed with Min-Max Scaling.\n",
    "\n",
    "Min-Max Scaling scales features to lie between a given minimum and maximum value, often between 0 and 1. This transformation preserves the shape of the original distribution and doesn't change the information embedded in the original data.\n",
    "\n",
    "Here's the Python code to apply Min-Max Scaling to the numerical features of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c3d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying numerical columns\n",
    "numerical_features = bigmart_data.select_dtypes(include=[float, int]).columns\n",
    "\n",
    "# Applying Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "bigmart_data[numerical_features] = scaler.fit_transform(bigmart_data[numerical_features])\n",
    "\n",
    "bigmart_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe05568",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335127b2",
   "metadata": {},
   "source": [
    "Feature engineering is the process of creating new features or modifying existing ones to improve model performance. It involves using domain knowledge and mathematical techniques to generate features that make the machine learning algorithm work more effectively.\n",
    "\n",
    "Let's explore some common feature engineering techniques and potential new features we could create for the Bigmart dataset:\n",
    "\n",
    "**Polynomial Features:** Creating interaction terms or powers of features. This can help in capturing non-linear relationships.\n",
    "\n",
    "**Binning:** Converting continuous variables into categorical ones by grouping values into bins or intervals.\n",
    "\n",
    "**Aggregation:** Aggregating data can help in generating higher-level features. For example, we could compute the average sales for each type of item or each outlet.\n",
    "\n",
    "**Domain-Specific Features:** Using domain knowledge to create new features. For example:\n",
    "\n",
    "Creating a feature to indicate whether an item is a snack based on its Item_Type.\n",
    "Creating a feature indicating if an outlet is relatively new or old based on its age.\n",
    "\n",
    "**Feature Decomposition:** Techniques like PCA (Principal Component Analysis) can be used to reduce dimensionality and create new features.\n",
    "\n",
    "**Time-based Features:** If we had timestamped data, we could create features like \"day of the week,\" \"hour of the day,\" etc.\n",
    "\n",
    "Given the Bigmart dataset, here are some potential feature engineering steps we could consider:\n",
    "\n",
    "**IsSnack:** A binary feature indicating whether an item is a snack or not based on its type.\n",
    "\n",
    "**Average Sales by Item Type:** Calculate the average sales for each item type.\n",
    "\n",
    "**Outlet Age Group:** Categorize outlets as \"New,\" \"Medium Age,\" or \"Old\" based on the Outlet_Age feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a44ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. IsSnack: Using the one-hot encoded columns\n",
    "bigmart_data['IsSnack'] = bigmart_data.apply(lambda row: 1 if row['Item_Type_Breads'] == 1 or row['Item_Type_Snack Foods'] == 1 else 0, axis=1)\n",
    "\n",
    "# 2. Average Sales by Item Type: We will use the original 'Item_Outlet_Sales' column to calculate this.\n",
    "average_sales = bigmart_data['Item_Outlet_Sales'].mean()\n",
    "bigmart_data['Avg_Sales_by_Item_Type'] = average_sales\n",
    "\n",
    "# 3. Outlet Age Group: Use the 'Outlet_Age' feature derived earlier.\n",
    "bins = [0, 10, 20, 35]  # Bins for new (0-10), medium age (10-20), and old (20-35)\n",
    "labels = ['New', 'Medium Age', 'Old']\n",
    "bigmart_data['Outlet_Age_Group'] = pd.cut(bigmart_data['Outlet_Age'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "bigmart_data[['IsSnack', 'Avg_Sales_by_Item_Type', 'Outlet_Age_Group']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a45bae",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85079ca9",
   "metadata": {},
   "source": [
    "Feature selection is the process of reducing the number of input variables when developing a predictive model. Proper feature selection can lead to simpler, more interpretable models that perform better on new data.\n",
    "\n",
    "There are various methods for feature selection:\n",
    "\n",
    "**Filter Methods:** These are based on the general characteristics of data. They use statistical measures to score each feature. Common techniques include:\n",
    "\n",
    "**Correlation Coefficient:** Removing features that are highly correlated.\n",
    "\n",
    "**Chi-Squared Test:** Used for categorical data to test the relationship between each feature and the target.\n",
    "\n",
    "**Wrapper Methods:** These use a subset of features and train a model using them. Based on the inferences drawn from the previous model, they decide to add or remove features. \n",
    "\n",
    "Common techniques include:\n",
    "\n",
    "**Recursive Feature Elimination (RFE):** A type of wrapper method.\n",
    "\n",
    "**Embedded Methods:** These are iterative in a sense that takes care of each iteration of the model training process and carefully extracts those features that contribute the most to the training for a particular iteration. Common techniques include:\n",
    "\n",
    "**LASSO Regression:** Adds penalty equivalent to the absolute value of the magnitude of coefficients.\n",
    "\n",
    "**Tree-based Methods:** Like Decision Trees, Random Forests, and Gradient Boosted Trees can be used to rank feature importance.\n",
    "\n",
    "For the Bigmart dataset and considering its size and the number of features, I'd recommend starting with:\n",
    "\n",
    "**Correlation Coefficient:** To remove features that are highly correlated.\n",
    "\n",
    "**Tree-based Methods:** Using a tree-based method like a Random Forest to rank and select the most important features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f94f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "correlation_matrix = bigmart_data.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c576cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the columns and the number of occurrences of ' --'\n",
    "occurrences = bigmart_data[bigmart_data == '  --'].count()\n",
    "occurrences = occurrences[occurrences > 0]\n",
    "occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e4a1f",
   "metadata": {},
   "source": [
    "The value ' --' in Outlet_Location_Type indicates missing or unrecorded data. We have a few options to handle this:\n",
    "\n",
    "**Remove Rows:** If the number of rows with ' --' is small relative to the dataset's size, we could remove these rows.\n",
    "**Impute with Mode:** Replace ' --' with the most common value (mode) in the Outlet_Location_Type column. This is a typical method for handling missing categorical data.\n",
    "**Create a New Category:** Treat ' --' as a separate category. This might be useful if ' --' has a special meaning, but it can introduce noise if it's merely a placeholder for missing data.\n",
    "\n",
    "Given our previous discussions and data preprocessing steps, I'd recommend imputing with the mode. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1750b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute ' --' with the mode in the 'Outlet_Location_Type' column\n",
    "mode_value = bigmart_data['Outlet_Location_Type'].mode()[0]\n",
    "bigmart_data['Outlet_Location_Type'].replace('  --', mode_value, inplace=True)\n",
    "\n",
    "# Checking if ' --' has been replaced\n",
    "remaining_occurrences = bigmart_data[bigmart_data == '  --'].count().sum()\n",
    "remaining_occurrences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cadd33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "690155f5",
   "metadata": {},
   "source": [
    "After examining the correlation matrix and removing any features that are highly correlated, we will use the Random Forest algorithm to rank and select the most important features.\n",
    "\n",
    "## Tree-based Method: Random Forest\n",
    "Random Forest is an ensemble learning method that can be used for both classification and regression tasks. It works by constructing multiple decision trees during training and outputs the class that is the mode of the classes (classification) or the mean prediction (regression) of the individual trees.\n",
    "\n",
    "Let's proceed with using a Random Forest to rank the importance of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for 'Outlet_Location_Type'\n",
    "one_hot_encoded_data = pd.get_dummies(bigmart_data['Outlet_Location_Type'], drop_first=True, prefix='Outlet_Location_Type')\n",
    "bigmart_data = pd.concat([bigmart_data, one_hot_encoded_data], axis=1)\n",
    "bigmart_data.drop('Outlet_Location_Type', axis=1, inplace=True)\n",
    "\n",
    "# One-hot encoding for 'Outlet_Age_Group'\n",
    "one_hot_encoded_age_group = pd.get_dummies(bigmart_data['Outlet_Age_Group'], drop_first=True, prefix='Outlet_Age_Group')\n",
    "bigmart_data = pd.concat([bigmart_data, one_hot_encoded_age_group], axis=1)\n",
    "bigmart_data.drop('Outlet_Age_Group', axis=1, inplace=True)\n",
    "\n",
    "# Separating features and target variable\n",
    "X = bigmart_data.drop('Item_Outlet_Sales', axis=1)\n",
    "y = bigmart_data['Item_Outlet_Sales']\n",
    "\n",
    "# Initializing and fitting the Random Forest model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Getting feature importances\n",
    "feature_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "\n",
    "# Sorting feature importances in descending order and plotting\n",
    "feature_importances_sorted = feature_importances.sort_values(ascending=False)\n",
    "feature_importances_sorted.plot(kind='barh', figsize=(12, 10), color='skyblue')\n",
    "plt.title('Feature Importances using Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16adfe2e",
   "metadata": {},
   "source": [
    "Based on the feature importance plot generated by the Random Forest model, we can make a decision on which features to discard.\n",
    "\n",
    "Typically, features with the least importance (those at the bottom of the plot) contribute the least to the model's predictive power and can be considered for removal.\n",
    "\n",
    "Here are the steps to discard less important features:\n",
    "\n",
    "Set a threshold: Decide on a threshold below which feature importance is considered low.\n",
    "\n",
    "List features below the threshold: Identify features whose importance is below the set threshold.\n",
    "\n",
    "Remove those features from the dataset.\n",
    "\n",
    "Let's determine which features fall below a certain threshold of importance. For demonstration purposes, I'll set a threshold that captures features with an importance below 0.01, but this can be adjusted based on your preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c900ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = bigmart_data.drop('Item_Outlet_Sales', axis=1)\n",
    "y = bigmart_data['Item_Outlet_Sales']\n",
    "\n",
    "# Train Random Forest and get feature importances\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "feature_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "\n",
    "# Discard features with importance below 0.01\n",
    "features_to_discard = feature_importances[feature_importances < 0.01].index.tolist()\n",
    "bigmart_data_reduced = bigmart_data.drop(features_to_discard, axis=1)\n",
    "\n",
    "# Display the columns that were removed\n",
    "features_to_discard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cf8f5d",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5088c33",
   "metadata": {},
   "source": [
    "The next step in the pipeline is to split the dataset into a training set and a testing set. This allows us to train a machine learning model on one subset and validate its performance on another, unseen subset.\n",
    "\n",
    "Typically, we use about 70-80% of the data for training and the remaining 20-30% for testing. However, this ratio can vary based on the dataset size and the specific problem at hand.\n",
    "\n",
    "I'll proceed to split the data into 80% for training and 20% for testing. If you have a different preference for the split ratio or any other specifics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532db186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X = bigmart_data_reduced.drop('Item_Outlet_Sales', axis=1)\n",
    "y = bigmart_data['Item_Outlet_Sales']\n",
    "\n",
    "# Splitting the data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8141157d",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7d9cae",
   "metadata": {},
   "source": [
    "For model building, we have a plethora of algorithms to choose from. Given the nature of the dataset and the task (predicting sales, which is a regression problem), here are some commonly used regression algorithms:\n",
    "\n",
    "**Linear Regression:** A simple linear approach to establish a relationship between the dependent and independent variables. It's a good starting point.\n",
    "\n",
    "**Decision Trees and Random Forest:** Trees partition the data space into regions, and Random Forest is an ensemble of such trees. They can capture non-linear relationships.\n",
    "\n",
    "**Gradient Boosted Trees (e.g., XGBoost, LightGBM):** These are boosting algorithms that build trees in a sequential manner, where each tree tries to correct the errors of the previous one.\n",
    "\n",
    "**Support Vector Regression:** Uses the support vector machine (SVM) algorithm for regression.\n",
    "\n",
    "**Neural Networks:** Multi-layer perceptrons can be used for regression tasks and can capture complex, non-linear relationships.\n",
    "\n",
    "As a starting point, I'd recommend beginning with a simple Linear Regression to establish a baseline, then trying more complex models like Random Forest or Gradient Boosted Trees to see if we can improve performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce46711",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d88675",
   "metadata": {},
   "source": [
    "let's proceed with building a Linear Regression model.\n",
    "\n",
    "Here are the steps:\n",
    "\n",
    "Initialize the Linear Regression model.\n",
    "\n",
    "Train (fit) the model using the training data.\n",
    "\n",
    "Evaluate the model's performance on the training data and the testing data using metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4af8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X = bigmart_data_reduced.drop('Item_Outlet_Sales', axis=1)\n",
    "y = bigmart_data['Item_Outlet_Sales']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "train_predictions = lr_model.predict(X_train)\n",
    "test_predictions = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mae_train = mean_absolute_error(y_train, train_predictions)\n",
    "mse_train = mean_squared_error(y_train, train_predictions)\n",
    "r2_train = r2_score(y_train, train_predictions)\n",
    "\n",
    "mae_test = mean_absolute_error(y_test, test_predictions)\n",
    "mse_test = mean_squared_error(y_test, test_predictions)\n",
    "r2_test = r2_score(y_test, test_predictions)\n",
    "\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_train}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_train}\")\n",
    "print(f\"R-squared: {r2_train}\\n\")\n",
    "\n",
    "print(\"Testing Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_test}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_test}\")\n",
    "print(f\"R-squared: {r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287d8837",
   "metadata": {},
   "source": [
    "After executing the code, you'll get the performance metrics of the Linear Regression model for both the training and testing datasets.\n",
    "\n",
    "This will provide you with a sense of how well the model fits the training data and how well it generalizes to new, unseen data. The R-squared value will give an indication of the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "\n",
    "Once you've done this, you can decide if you want to explore more complex models or tune the current one further. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174a2077",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e280af2",
   "metadata": {},
   "source": [
    "Let's proceed with building a Random Forest model.\n",
    "\n",
    "The steps involved are:\n",
    "\n",
    "Initialize the Random Forest Regressor.\n",
    "\n",
    "Train (fit) the model using the training data.\n",
    "\n",
    "Evaluate the model's performance on the training data and the testing data using metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared.\n",
    "\n",
    "Let me provide you with the code to execute these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# 2. Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predictions\n",
    "rf_train_predictions = rf_model.predict(X_train)\n",
    "rf_test_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "rf_mae_train = mean_absolute_error(y_train, rf_train_predictions)\n",
    "rf_mse_train = mean_squared_error(y_train, rf_train_predictions)\n",
    "rf_r2_train = r2_score(y_train, rf_train_predictions)\n",
    "\n",
    "rf_mae_test = mean_absolute_error(y_test, rf_test_predictions)\n",
    "rf_mse_test = mean_squared_error(y_test, rf_test_predictions)\n",
    "rf_r2_test = r2_score(y_test, rf_test_predictions)\n",
    "\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {rf_mae_train}\")\n",
    "print(f\"Mean Squared Error (MSE): {rf_mse_train}\")\n",
    "print(f\"R-squared: {rf_r2_train}\\n\")\n",
    "\n",
    "print(\"Testing Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {rf_mae_test}\")\n",
    "print(f\"Mean Squared Error (MSE): {rf_mse_test}\")\n",
    "print(f\"R-squared: {rf_r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f945f1",
   "metadata": {},
   "source": [
    "After executing the code, you'll get the performance metrics of the Random Forest model for both the training and testing datasets. This will help you understand the model's fit to the training data and its generalization to new data.\n",
    "\n",
    "Once you have the results, you can compare the performance with the Linear Regression model. If Random Forest offers significant improvement, you might consider fine-tuning its hyperparameters for even better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c869cddb",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a1e1a1",
   "metadata": {},
   "source": [
    "Alright, if Random Forest didn't provide a significant improvement, the next model I'd recommend trying is the Gradient Boosted Trees model, specifically using XGBoost.\n",
    "\n",
    "XGBoost is a popular gradient boosting library that often performs well in machine learning competitions and real-world datasets.\n",
    "\n",
    "Here's how you can train and evaluate an XGBoost regressor on your dataset:\n",
    "\n",
    "Install the XGBoost library (if you haven't already).\n",
    "\n",
    "Initialize the XGBoost regressor.\n",
    "\n",
    "Train (fit) the model using the training data.\n",
    "\n",
    "Evaluate the model's performance using the same metrics as before.\n",
    "\n",
    "Let's proceed with these steps.\n",
    "\n",
    "To train and evaluate an XGBoost regressor, follow these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504cf130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost regressor\n",
    "xgb_model = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "xgb_train_predictions = xgb_model.predict(X_train)\n",
    "xgb_test_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "xgb_mae_train = mean_absolute_error(y_train, xgb_train_predictions)\n",
    "xgb_mse_train = mean_squared_error(y_train, xgb_train_predictions)\n",
    "xgb_r2_train = r2_score(y_train, xgb_train_predictions)\n",
    "\n",
    "xgb_mae_test = mean_absolute_error(y_test, xgb_test_predictions)\n",
    "xgb_mse_test = mean_squared_error(y_test, xgb_test_predictions)\n",
    "xgb_r2_test = r2_score(y_test, xgb_test_predictions)\n",
    "\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {xgb_mae_train}\")\n",
    "print(f\"Mean Squared Error (MSE): {xgb_mse_train}\")\n",
    "print(f\"R-squared: {xgb_r2_train}\\n\")\n",
    "\n",
    "print(\"Testing Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {xgb_mae_test}\")\n",
    "print(f\"Mean Squared Error (MSE): {xgb_mse_test}\")\n",
    "print(f\"R-squared: {xgb_r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dfaf78",
   "metadata": {},
   "source": [
    "If the training performance is good but the testing performance is lagging, it indicates that the model might be overfitting. Overfitting happens when the model learns the training data too closely, including its noise and outliers, and thus performs poorly on unseen data.\n",
    "\n",
    "Here are a few strategies to address overfitting in XGBoost:\n",
    "\n",
    "**Increase Regularization:** XGBoost provides parameters like alpha (L1 regularization) and lambda (L2 regularization) that can be adjusted to add regularization to the model.\n",
    "\n",
    "**Reduce Model Complexity:** Reduce the depth of the trees by setting a lower value for the max_depth parameter.\n",
    "\n",
    "**Add Early Stopping:** Train the model for a large number of rounds and use early stopping to stop training once the model's performance on a validation set stops improving.\n",
    "\n",
    "**Reduce Learning Rate:** A smaller learning rate with a higher number of boosting rounds can sometimes help in getting a more generalized model.\n",
    "\n",
    "**Feature Selection:** Ensure that only relevant features are being used. Irrelevant or redundant features can lead to overfitting.\n",
    "\n",
    "Let's start by implementing some of these strategies with XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9042edcb",
   "metadata": {},
   "source": [
    "Alright, let's start by implementing a combination of the strategies mentioned to combat overfitting in the XGBoost model:\n",
    "\n",
    "**Reduce Learning Rate:** We'll set a smaller learning rate and increase the number of boosting rounds.\n",
    "\n",
    "**Add Early Stopping:** We'll use early stopping with a validation set to prevent overfitting.\n",
    "\n",
    "**Increase Regularization:** We'll add L1 and L2 regularization to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df80c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the training data further into training and validation sets for early stopping\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Initialize the XGBoost regressor with adjusted parameters\n",
    "xgb_model_adjusted = xgb.XGBRegressor(\n",
    "    objective ='reg:squarederror', \n",
    "    n_estimators=1000,  # Increase boosting rounds\n",
    "    learning_rate=0.01,  # Reduce learning rate\n",
    "    reg_alpha=10,  # L1 regularization\n",
    "    reg_lambda=10,  # L2 regularization\n",
    "    max_depth=4,  # Reduce model complexity\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "xgb_model_adjusted.fit(\n",
    "    X_train_sub, y_train_sub, \n",
    "    early_stopping_rounds=10, \n",
    "    eval_set=[(X_val, y_val)], \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "xgb_train_predictions_adjusted = xgb_model_adjusted.predict(X_train)\n",
    "xgb_test_predictions_adjusted = xgb_model_adjusted.predict(X_test)\n",
    "\n",
    "# Evaluate the adjusted model's performance\n",
    "xgb_mae_train_adjusted = mean_absolute_error(y_train, xgb_train_predictions_adjusted)\n",
    "xgb_mse_train_adjusted = mean_squared_error(y_train, xgb_train_predictions_adjusted)\n",
    "xgb_r2_train_adjusted = r2_score(y_train, xgb_train_predictions_adjusted)\n",
    "\n",
    "xgb_mae_test_adjusted = mean_absolute_error(y_test, xgb_test_predictions_adjusted)\n",
    "xgb_mse_test_adjusted = mean_squared_error(y_test, xgb_test_predictions_adjusted)\n",
    "xgb_r2_test_adjusted = r2_score(y_test, xgb_test_predictions_adjusted)\n",
    "\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {xgb_mae_train_adjusted}\")\n",
    "print(f\"Mean Squared Error (MSE): {xgb_mse_train_adjusted}\")\n",
    "print(f\"R-squared: {xgb_r2_train_adjusted}\\n\")\n",
    "\n",
    "print(\"Testing Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {xgb_mae_test_adjusted}\")\n",
    "print(f\"Mean Squared Error (MSE): {xgb_mse_test_adjusted}\")\n",
    "print(f\"R-squared: {xgb_r2_test_adjusted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb6819",
   "metadata": {},
   "source": [
    "Running the above code will train an XGBoost model with the suggested adjustments and provide performance metrics for both training and testing datasets.\n",
    "\n",
    "After evaluating the results, we can decide if further hyperparameter tuning is necessary or if we should explore a different model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162ff2d8",
   "metadata": {},
   "source": [
    "The fact that the training and testing scores are now closer suggests that the overfitting issue has been somewhat addressed. However, the performance might still not be satisfactory. Here are some potential next steps:\n",
    "\n",
    "**Hyperparameter Tuning:** Use tools like GridSearchCV or RandomizedSearchCV from Scikit-learn to systematically search for the best hyperparameters for the XGBoost model.\n",
    "\n",
    "**Try Different Models:** As previously mentioned, you could explore models like Support Vector Regression (SVR), Neural Networks, or Gradient Boosted Trees using libraries like LightGBM or CatBoost.\n",
    "\n",
    "**Feature Engineering:** Revisit the features in the dataset. Creating new features or transforming existing ones can sometimes help in improving model performance.\n",
    "\n",
    "**Data Collection:** If possible, gather more data. A larger dataset might help in improving the model's generalization.\n",
    "\n",
    "**Model Stacking:** Combine predictions from multiple models to improve performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75ccf8",
   "metadata": {},
   "source": [
    "## GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06781b5a",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is a crucial step in improving model performance. We'll use GridSearchCV to systematically search for the best hyperparameters for the XGBoost model.\n",
    "\n",
    "Here's the plan:\n",
    "\n",
    "Define a set of hyperparameters to search.\n",
    "\n",
    "Use GridSearchCV to evaluate all the possible combinations of hyperparameters.\n",
    "\n",
    "Train the XGBoost model with the best hyperparameters found.\n",
    "\n",
    "Evaluate the model's performance.\n",
    "\n",
    "\n",
    "Let's proceed with this approach.\n",
    "\n",
    "Here's how you can perform hyperparameter tuning using GridSearchCV with XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f0c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters and their possible values\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'subsample': [0.5, 0.7, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.7, 1.0],\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'reg_alpha': [1e-5, 1e-2, 0.1, 1, 100],\n",
    "    'reg_lambda': [1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "\n",
    "# Initialize XGBoost regressor\n",
    "xgb_model_for_gs = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(xgb_model_for_gs, param_grid, cv=3, n_jobs=-1, verbose=1, scoring='r2')\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_parameters = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Predictions using the model with the best parameters\n",
    "xgb_gs_train_predictions = best_estimator.predict(X_train)\n",
    "xgb_gs_test_predictions = best_estimator.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "xgb_gs_mae_train = mean_absolute_error(y_train, xgb_gs_train_predictions)\n",
    "xgb_gs_mse_train = mean_squared_error(y_train, xgb_gs_train_predictions)\n",
    "xgb_gs_r2_train = r2_score(y_train, xgb_gs_train_predictions)\n",
    "\n",
    "xgb_gs_mae_test = mean_absolute_error(y_test, xgb_gs_test_predictions)\n",
    "xgb_gs_mse_test = mean_squared_error(y_test, xgb_gs_test_predictions)\n",
    "xgb_gs_r2_test = r2_score(y_test, xgb_gs_test_predictions)\n",
    "\n",
    "print(\"Best Parameters:\", best_parameters)\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {xgb_gs_mae_train}\")\n",
    "print(f\"Mean Squared Error (MSE): {xgb_gs_mse_train}\")\n",
    "print(f\"R-squared: {xgb_gs_r2_train}\\n\")\n",
    "\n",
    "print(\"Testing Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {xgb_gs_mae_test}\")\n",
    "print(f\"Mean Squared Error (MSE): {xgb_gs_mse_test}\")\n",
    "print(f\"R-squared: {xgb_gs_r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a351fb",
   "metadata": {},
   "source": [
    "This script will search over the defined hyperparameters using cross-validation. Once the best parameters are identified, the model will be evaluated on the training and testing data.\n",
    "\n",
    "Please note: Grid search can be computationally intensive, especially with a large grid like the one defined. Depending on your computational resources, this process might take some time.\n",
    "\n",
    "After executing the script, compare the performance metrics with the previous models to determine if there's an improvement. If the results are still unsatisfactory, further tuning or trying a different model might be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ace6e7a",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264eb615",
   "metadata": {},
   "source": [
    "LightGBM is another gradient boosting framework that uses tree-based learning algorithms, and it's known for its efficiency and speed. It can handle large datasets and is optimized for performance.\n",
    "\n",
    "Let's proceed with building a LightGBM model for your dataset:\n",
    "\n",
    "Install the LightGBM library (if you haven't already).\n",
    "                              \n",
    "Initialize the LightGBM regressor.\n",
    "                              \n",
    "Train (fit) the model using the training data.\n",
    "                              \n",
    "Evaluate the model's performance using metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4e5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LightGBM regressor\n",
    "lgb_model = lgb.LGBMRegressor(objective='regression', n_estimators=1000, learning_rate=0.01, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "lgb_train_predictions = lgb_model.predict(X_train)\n",
    "lgb_test_predictions = lgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "lgb_mae_train = mean_absolute_error(y_train, lgb_train_predictions)\n",
    "lgb_mse_train = mean_squared_error(y_train, lgb_train_predictions)\n",
    "lgb_r2_train = r2_score(y_train, lgb_train_predictions)\n",
    "\n",
    "lgb_mae_test = mean_absolute_error(y_test, lgb_test_predictions)\n",
    "lgb_mse_test = mean_squared_error(y_test, lgb_test_predictions)\n",
    "lgb_r2_test = r2_score(y_test, lgb_test_predictions)\n",
    "\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {lgb_mae_train}\")\n",
    "print(f\"Mean Squared Error (MSE): {lgb_mse_train}\")\n",
    "print(f\"R-squared: {lgb_r2_train}\\n\")\n",
    "\n",
    "print(\"Testing Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {lgb_mae_test}\")\n",
    "print(f\"Mean Squared Error (MSE): {lgb_mse_test}\")\n",
    "print(f\"R-squared: {lgb_r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd809367",
   "metadata": {},
   "source": [
    "After executing this code, you'll get the performance metrics of the LightGBM model for both the training and testing datasets. If the LightGBM model provides an improvement, further hyperparameter tuning or model adjustments can be explored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e1e2f",
   "metadata": {},
   "source": [
    "## Model Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3028057",
   "metadata": {},
   "source": [
    "Model stacking is a technique where you use the predictions from multiple models to train a meta-model (or blender) that then makes the final prediction. The idea is to take advantage of the strengths of each individual model to produce a better and more generalized prediction.\n",
    "\n",
    "Here's a simplified approach to model stacking:\n",
    "\n",
    "Split the training set into two subsets.\n",
    "\n",
    "Train multiple models on the first subset.\n",
    "\n",
    "Use the trained models to make predictions on the second subset.\n",
    "\n",
    "Use the predictions from step 3 as input features to train a meta-model.\n",
    "\n",
    "Use the meta-model to make predictions on the test set.\n",
    "\n",
    "For this example, we'll use three base models:\n",
    "\n",
    "Linear Regression\n",
    "\n",
    "Random Forest Regressor\n",
    "\n",
    "XGBoost Regressor\n",
    "\n",
    "And our meta-model will be a simple Linear Regression.\n",
    "\n",
    "Let's proceed with this approach.\n",
    "\n",
    "Here's how you can implement model stacking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5ad301",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sub1, X_train_sub2, y_train_sub1, y_train_sub2 = train_test_split(X_train, y_train, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce298953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "lr = LinearRegression()\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "xg = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators=100, random_state=42)\n",
    "\n",
    "# Train models\n",
    "lr.fit(X_train_sub1, y_train_sub1)\n",
    "rf.fit(X_train_sub1, y_train_sub1)\n",
    "xg.fit(X_train_sub1, y_train_sub1)\n",
    "\n",
    "lr_pred = lr.predict(X_train_sub2)\n",
    "rf_pred = rf.predict(X_train_sub2)\n",
    "xg_pred = xg.predict(X_train_sub2)\n",
    "\n",
    "stacked_predictions = np.column_stack((lr_pred, rf_pred, xg_pred))\n",
    "\n",
    "# Train meta-model\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(stacked_predictions, y_train_sub2)\n",
    "\n",
    "lr_test_pred = lr.predict(X_test)\n",
    "rf_test_pred = rf.predict(X_test)\n",
    "xg_test_pred = xg.predict(X_test)\n",
    "\n",
    "stacked_test_predictions = np.column_stack((lr_test_pred, rf_test_pred, xg_test_pred))\n",
    "final_predictions = meta_model.predict(stacked_test_predictions)\n",
    "\n",
    "\n",
    "mae = mean_absolute_error(y_test, final_predictions)\n",
    "mse = mean_squared_error(y_test, final_predictions)\n",
    "r2 = r2_score(y_test, final_predictions)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e0d8dd",
   "metadata": {},
   "source": [
    "This approach provides a step-by-step guideline for stacking. In practice, the choice of base models and the meta-model can be varied based on the problem and data. Model stacking can be a powerful technique, but it also increases the complexity of the solution, so careful consideration is needed when deploying such models.\n",
    "\n",
    "After executing the code, evaluate the performance metrics of the stacked model against the individual models to see if there's an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48931c09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
