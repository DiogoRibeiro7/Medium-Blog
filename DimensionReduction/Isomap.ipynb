{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ISOMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Isomap stands for isometric mapping. Isomap is a non-linear dimensionality reduction method based on spectral theory which tries to preserve the geodesic distances in the lower dimension. Isomap starts by creating a neighborhood network. After that, it uses graph distance to approximate geodesic distance between all pairs of points. And then, through  eigenvalue decomposition of geodesic distance matrix it finds the low dimensional embedding of the dataset. In  non-linear manifolds euclidean metric for distance holds good if and only if neigborhood structure can be approximated as linear. If neighborhood contains holes, then euclidean distances can be highly misleading. In contrast to this, if we measure the distance between two points by following the manifold, we will have a better approximation of how far or near two points are. \n",
    "Let's understand this with a extremely simple 2-D example. Suppose our data lies on a circular manifold in a 2-D structure like in the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why geodesic distances are better than euclidean distances in nonlinear manifolds?\n",
    "![pic](isomap_explain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reduce the data to 1-D using euclidean distances and approximate geodesic distances. Now, if we look at \n",
    "the 1-D mapping based on the euclidean metric, we see that for points which are far apart(`a & b`) have been mapped poorly. Only the points which can be approximated to lie on a linear manifold(`c & d`) give satisfactory results. On the other hand see the mapping with geodesic distances, it nicely approximates the close points as neighbors and far away points as distant.   \n",
    "The geodesic distances between two points in the image is approximated by graph distance between the two points. Thus, euclidean distances should not be used for appoximating distance between two points in non-linear manifolds while geodesic distances can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isomap uses the above principle to create a similarity matrix for eigenvalue decomposition. Unlike other non-linear dimensionality reduction like `LLE & LPP` which only use local information, isomap uses the local information to create a global similarity matrix. Isomap algorithm uses euclidean metrics to prepare the neighborhood graph. Then , it approximates the gedoesic distance between two points by measuring shortest path between these points using graph distance. Thus, it approximates both global as well as local structure of the dataset in the low dimensional embedding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a basic understanding of few concepts which we need to implement Isomap algorithm.  \n",
    "**Pregel API** - Pregel is a distributed programming model developed by google for processing large scale graphs . It is the inspiration behind the Apache giraph project and GraphX library of spark. Pregel is basically a message-passing interface based on a idea that a vertex's state should depend on its neighbors. A pregel computation takes as input a graph and a set of vertex states. At every iteration which is called *superstep* it processes messages received at a vertex and updates the vertex state. After that it decides which of it's neighbors should receive message at next superstep and what should be the message. Thus, messages are passed along edges and computation happens only at the vertices. Graph is not passed across the network only messages. Computation stops at maximum iterations or when no messages are left to pass.  Let's understand it using a simple example.\n",
    "Suppose, We need to find the degree of each vertex for the graph given below. Image shown below represents a single iteration of pregel model. At initialization, every vertex's degree is `0`. We can send an empty message as initial message to start the computation. At the end of superstep 1, Each vertex sends  message `1` through each of its edges. At next superstep each vertex sums the messages received and update its degree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pregel](pregel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Classical MDS** - Isomap is closely related to the original multi dimensional scaling algorithm proposed by the Torgerson and Gower. In fact, it is an extension of the classical multidimensional scaling. Classical multidimensional algorithm gives a closed form solution to the dimensionality reduction problem.  Classical MDS uses the euclidean distances as the similairty metric while isomap uses geodesic distances.\n",
    "Steps of classical MDS are\n",
    "1. Create matrix of squared dissimilarities $\\Delta^2(X)$ from the given X.  \n",
    "2. Obtain the matrix $B$ by double centring the dissimialrity matrix $B = -\\frac{1}{2}J \\Delta^2 J$\n",
    "2. Compute the eigenvalue decomposition of matrix B, $B_{\\Delta} = Q\\Lambda Q^`$ \n",
    "3. Choose the K eigenvectors having K highest eigenvalues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps of IsoMaps\n",
    "1. Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Imports \n",
    "// 1. from the spark module\n",
    "import org.apache.spark.sql.{functions => funcs}\n",
    "import org.apache.spark.ml.feature.{StandardScaler, MaxAbsScaler}\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.ml.linalg.{ Vectors => mlVs, Vector => mlV, DenseVector => mlDV}\n",
    "import org.apache.spark.mllib.linalg.{Vector => mllibV, Vectors => mllibVs}\n",
    "import org.apache.spark.mllib.linalg.distributed.{IndexedRowMatrix, IndexedRow}\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "// scala language modules\n",
    "import scala.collection.immutable.{Map => imMap, TreeMap => TMap}\n",
    "import scala.{math => m}\n",
    "\n",
    "// vegas-viz for visualization\n",
    "import vegas._\n",
    "import vegas.data.External._\n",
    "\n",
    "// mathematical computation library Breeze modules\n",
    "import breeze.linalg._\n",
    "import breeze.linalg.{DenseVector => BDV, DenseMatrix => BDM}\n",
    "import breeze.stats._  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isomap differs from classical MDS in intial few steps only. Instead of using euclidean metric for dissimialrity, it uses graph distances. Steps of Isomap algorithm\n",
    "1. **Neighborhood search** -\n",
    "Neighbourhood can be created  through k-nearest neighbor or $\\epsilon$-ball neighborhood approach.  \n",
    "**K-nearest neighbor** - Each point is connected to its K-nearest points. Using this appraoch we will always have K-neighbors for each and every point. Since a point selects exactly K-points and it may be selected by some other point as neighbor which is not in his neighborhood set. This situation generally arises in case of an isolated point which selects faraway points as neighbors, while these neighbors can select neighborhood set from smaller distance. This produces asymmetric neighborhood matrix.  \n",
    "**$\\epsilon$-ball neighbor** - Each point $Y_i$ selects every point inside the ball with radius $\\epsilon$ and centered at $Y_i$ as its neighbors. This approach sometimes leads to points with no neighbors. It is hard to find the right $\\epsilon$, since smaller value will give many isolated points and higher value will have many neighbors for each points. This approach is good for approximating geodesic distances.  \n",
    "While creating neighborhood matrix, we have to make sure that the every neighborhood graphs is connected to atleast one other neighborhood graph. If two neighborhoods don't have a common points among them, then we will have disconnected compoenents in graph and dissimilarity matrix will remain incomplete .  \n",
    "Now we have to assign weight to each edge for creating an adjacency matrix from the neighborhood graph. If we need an unweighted graph we can choose all edge weights equal to 1, otherwise we can use euclidean distances between the points as weights for the weighted graphs.  \n",
    "We will be using $\\epsilon$-Ball neighborhood approach with $\\epsilon$=0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//read the dataset \n",
    "val n_samples =150\n",
    "val n_dim = 2\n",
    "val n_bc = sc.broadcast(n_samples)\n",
    "\n",
    "val convertUDF = funcs.udf((array : Seq[Double]) => {mlVs.dense(array.toArray)})\n",
    "\n",
    "var rdd = sc.textFile(\"iris.csv\").filter(_(0).isDigit).map(_.split(\",\").take(4)).\n",
    "                                  map(_.map(_.toDouble)).zipWithIndex()\n",
    "var df = spark.createDataFrame(rdd ).toDF(\"features\",\"id\").\n",
    "                                withColumn(\"features\",convertUDF(funcs.col(\"features\")))\n",
    "\n",
    "// scale the dataset \n",
    "val MaxScaler = new MaxAbsScaler().setInputCol(\"features\").\n",
    "                                setOutputCol(\"scaled_features\")\n",
    "val model_abs = MaxScaler.fit(df)\n",
    "\n",
    "df = model_abs.transform(df)\n",
    "df.cache()\n",
    "\n",
    "df = df.drop(\"features\").select(funcs.col(\"id\"), funcs.col(\"scaled_features\").alias(\"features\"))\n",
    "\n",
    "df = df.crossJoin(df.select(funcs.col(\"id\").alias(\"id2\"),funcs.col(\"features\").alias(\"features2\")))\n",
    "\n",
    "val udf_dist = funcs.udf((x:mlV, y:mlV) => m.sqrt(mlVs.sqdist(x,y)))\n",
    "\n",
    "df = df.withColumn(\"dist\", udf_dist(funcs.col(\"features\"),funcs.col(\"features2\"))).\n",
    "                                 drop(\"features\",\"features2\")\n",
    "\n",
    "val epsilon = 0.5\n",
    "val edge_bool = funcs.udf((x:Double, y:Double) => (x < y))\n",
    "\n",
    "df = df.filter(edge_bool(funcs.col(\"dist\"), funcs.lit(epsilon)))\n",
    "\n",
    "df.cache()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Create the similariity matrix\n",
    "After neighborhood search, we will use spark's graphX library for calculating the geodesic distances between the points. While creating our neighborhood network, we have to make sure that the resulting graph is a single connected component. If not, then our similarity matrix will remain incomplete and results will be incoherent. We need to iterate over the different values of neighborhood selection parameter to get the fully connected graph. As of now, spark does not have a shortestpath function for the weighted graph. We will have to implement it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ShortestPath(Verts: RDD[(VertexId, imMap[Long, Double])], \n",
    "                 Edges: RDD[Edge[Double]], landmarks: Seq[Long] = Seq()): \n",
    "                                                        Graph[imMap[Long,Double],Double] = {\n",
    "\n",
    "        \n",
    "        val g = Graph(Verts, Edges)\n",
    "\n",
    "        type SPMap = Map[VertexId, Double]\n",
    " \n",
    "\n",
    "        def makeMap(x: (VertexId, Double)*) = Map(x: _*)\n",
    "\n",
    "        def incrementMap(spmap1: SPMap, spmap2: SPMap, d: Double): SPMap = {\n",
    "            spmap1.map { case (k, v) => \n",
    "                if (v + d < spmap2.getOrElse(k, Double.MaxValue)) k -> (v + d)\n",
    "                else -1L -> 0.0\n",
    "        \n",
    "            }\n",
    "        \n",
    "        }\n",
    "\n",
    "        def addMaps(spmap1: SPMap, spmap2: SPMap): SPMap = {\n",
    "            (spmap1.keySet ++ spmap2.keySet).map {\n",
    "              k => k -> math.min(spmap1.getOrElse(k, Double.MaxValue), spmap2.getOrElse(k, Double.MaxValue))\n",
    "            }(collection.breakOut) // more efficient alternative to [[collection.Traversable.toMap]]\n",
    "        }\n",
    "        \n",
    "        var spGraph: Graph[imMap[Long,Double],Double]  = null\n",
    "        \n",
    "        if (landmarks.isEmpty){\n",
    "            spGraph = g.mapVertices { (vid, attr) => makeMap(vid -> 0)}\n",
    "        }\n",
    "        else{\n",
    "            spGraph = g.mapVertices { (vid, attr) => \n",
    "                                if (landmarks.contains(vid)) makeMap(vid -> 0) else makeMap()}\n",
    "        }                                      \n",
    "        \n",
    "        val initialMessage = makeMap()\n",
    "\n",
    "        def vertexProgram(id: VertexId, attr: SPMap, msg: SPMap): SPMap = {\n",
    "            addMaps(attr, msg)\n",
    "        }\n",
    "\n",
    "        def sendMessage(edge: EdgeTriplet[SPMap, Double]): Iterator[(VertexId, SPMap)] = {\n",
    "\n",
    "            val newAttr = incrementMap(edge.srcAttr, edge.dstAttr, edge.attr) - (-1)\n",
    "    \n",
    "            if (!newAttr.isEmpty) Iterator((edge.dstId, newAttr))\n",
    "            else Iterator.empty\n",
    "    \n",
    "        }\n",
    "\n",
    "        val h = Pregel(spGraph, initialMessage)(vertexProgram, sendMessage, addMaps)\n",
    "\n",
    "        return(h)\n",
    "}\n",
    "                                                        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our shortest path function accepts two arguements first, a graph RDD with map as a vertex attribute and weight as an edge attribute & second ,a sequence of vertex ids for which we need distance. `sendMessage` defines a function which decides whom to send the message in the current iteration. The `vertexProgram` does all the processing of messages that are received at a node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Eigenvalue decomposition of the similarity matrix\n",
    "Remember before eigenvalue decomposition, we have to square the distance and double centre the squared similarities matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "//code for full isomap algo\n",
    "\n",
    "\n",
    "//initial states of the vertices\n",
    "val states: Map[Long, Double] = imMap()\n",
    "\n",
    "// vertex RDD\n",
    "val Verts: RDD[(VertexId, imMap[Long, Double])] = sc.parallelize((0 to (n_samples-1)).toSeq.map(i => \n",
    "                                                                                        i.toLong -> states))\n",
    "// Edge RDD                                                                                        \n",
    "val Edges: RDD[Edge[Double]] = df.rdd.map(x => Edge(x.getLong(0), x.getLong(1), x.getDouble(2)))\n",
    "\n",
    "// create centering matrix\n",
    "def create_row( id:Int): mllibV = {\n",
    "    \n",
    "    var row = (1 to n_bc.value).map(i => (-1.0/n_bc.value)).toArray\n",
    "    row(id) = row(id)+1.0\n",
    "    return(mllibVs.dense(row))\n",
    "\n",
    "}\n",
    "\n",
    "val ids = sc.parallelize((0 to (n_bc.value -1)).toSeq)\n",
    "\n",
    "\n",
    "val udf_cmat = funcs.udf((id:Int) => create_row(id))\n",
    "val cMat = new IndexedRowMatrix(ids.map((id:Int) => IndexedRow(id, create_row(id)))).toBlockMatrix(10,10)\n",
    "\n",
    "\n",
    "// need to square the dissimilarities. negative and 0.5 is due to the formula for centering the dist^2 matrix. & \n",
    "// it's much easier to apply it here than afterwards\n",
    "\n",
    "val graph_dist = ShortestPath(Verts, Edges)\n",
    "val graph_verts = graph_dist.mapVertices((x:Long,y:imMap[Long,Double]) => \n",
    "                                            TMap(y.toSeq:_*).values.toArray.map(i => -0.5*i*i)).vertices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val graph_irm = new IndexedRowMatrix(graph_verts.map((x:Tuple2[Long, Array[Double]]) => \n",
    "                                            IndexedRow(x._1, mllibVs.dense(x._2)))).toBlockMatrix(10,10)\n",
    "\n",
    "// centered dist mat\n",
    "\n",
    "val cDistMat = (cMat.multiply(graph_irm)).multiply(cMat).toIndexedRowMatrix()\n",
    "\n",
    "val Svd = cDistMat.computeSVD(2, true)\n",
    "val U = Svd.U\n",
    "val s_bc = sc.broadcast(Svd.s.toArray)\n",
    "\n",
    "val Isomap = U.rows.map(x => (x.index,x.vector.toArray.zip(s_bc.value).map(t => t._1/t._2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualization of the embedded dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pic](isomap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LandMark-Isomap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we implemented above was the vanilla version of Isomap. It requires a lot of time and computing power.\n",
    "It has two bottlenecks first Calculation of dissimilarity matrix requires $O(N^2)$ operations where $N$ is the number of the samples and second calculation of pairiwise graph distances. If N is huge, which is true generally in case of big datasets, it becomes impractical.\n",
    "Solution to this problem is **Landmark Isomap**. Landmark isomap is based on landmark MDS. Landmark MDS selects a group of points termed as **Landmarks** and implements classical MDS on them. Based on the mapping obtained from classical MDS, remaining points are mapped in the low dimensional embedding using distance based triangulation.  \n",
    "Steps for Landmark classical scaling  \n",
    "1. Selects landmarks points $X_{landmarks}$\n",
    "2. Apply classical MDS on landmarks points  and obtain low dimensional emebedding $L_k$\n",
    "3. calculate $\\delta_{u}$ where $\\delta_{ui}$ is mean of $i_{th}$ row of dissimilarity matrix of landmark points.\n",
    "4. Given a vector $x_a$ calculate $\\delta_a$ where $\\delta_{ai}$ is the squared distance between the point $x_a$ and the landmark point $i$\n",
    "5. the low dimensional embedding for the $x_a$ is given by $y_a^{} = \\frac{1}{2}L_k^{-1}(\\delta_a - \\delta_u)$ where $L_k^{-1}$ is the penrose moore inverse of the $L_k$\n",
    "\n",
    "Selection of landmark points can be random or through a specific method. For obtaining a K-dimensional embedding at least K+1 landmark points are needed. For reasons related to the stability of the algorithm, number of landmark points chosen should be more than strict minimum.\n",
    "The accuracy of isometric mapping in landmark isomap does not suffer much due to approximation in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val r = scala.util.Random\n",
    "r.setSeed(100)\n",
    "val num_landmarks = 10\n",
    "val num_lmarks_bc = sc.broadcast(num_landmarks)\n",
    "val landmark_ids = ((1 to (2*num_landmarks)) map {i => r.nextInt(n_samples)}).toSet.take(num_landmarks)\n",
    "val states: Map[Long, Double] = imMap()\n",
    "\n",
    "val Verts: RDD[(VertexId, imMap[Long, Double])] = sc.parallelize((landmark_ids).toSeq.map(i => \n",
    "                                                                                        i.toLong -> states))\n",
    "                                                                                        \n",
    "val Edges: RDD[Edge[Double]] = df.rdd.map(x => Edge(x.getLong(0), x.getLong(1), x.getDouble(2)))\n",
    "\n",
    "val graph_dist = ShortestPath(Verts, Edges, landmark_ids.map(_.toLong).toSeq)\n",
    "\n",
    "val graph_verts = graph_dist.mapVertices((x:Long,y:imMap[Long,Double]) => \n",
    "                                            TMap(y.toSeq:_*).values.toArray).vertices\n",
    "\n",
    "// separate landmark and non_landmark ids\n",
    "val landmark_bc = sc.broadcast(landmark_ids)\n",
    "val df_graph_dist = spark.createDataFrame(graph_verts ).toDF(\"id\",\"dist\")\n",
    "\n",
    "val is_lmark = funcs.udf(( x:Long ) => landmark_bc.value.contains(x.toInt))\n",
    "\n",
    "val lmark_dist = df_graph_dist.filter(is_lmark(funcs.col(\"id\")))\n",
    "val nlmark_dist = df_graph_dist.filter(!is_lmark(funcs.col(\"id\")))\n",
    "\n",
    "\n",
    "//use breeze for linear algebra related calculation \n",
    "\n",
    "val lmark_dist_local:Array[Double] = TMap(lmark_dist.rdd.map((x:Row) => \n",
    "                    (x.getLong(0), x.getSeq(1))).collect().toSeq:_*).\n",
    "                                values.map(_.toArray.map((i:Double) => -0.5*i*i)).flatten.toArray\n",
    "\n",
    "\n",
    "\n",
    "val LmarkMat = new BDM(num_landmarks, num_landmarks, lmark_dist_local)\n",
    "\n",
    "// create centering matix for the local matrix\n",
    "\n",
    "def create_row( id:Int):Array[Double] = {\n",
    "    \n",
    "    var row = (0 to (num_lmarks_bc.value-1)).map(i => (-1.0/num_lmarks_bc.value)).toArray\n",
    "    row(id) = row(id)+1.0\n",
    "    return(row)\n",
    "}\n",
    "\n",
    "val cMat = new BDM(num_landmarks,num_landmarks, (0 to (num_lmarks_bc.value-1)).\n",
    "                               toSeq.map(create_row).toArray.flatten)\n",
    "\n",
    "val temp = cMat * LmarkMat\n",
    "val cLmarkDist = temp * cMat\n",
    "\n",
    "\n",
    "\n",
    "val eig_ = eigSym(cLmarkDist)\n",
    "val eigVals = eig_.eigenvalues\n",
    "val eigVecs = eig_.eigenvectors(::,0 to (n_dim-1))\n",
    "\n",
    "                        \n",
    "val n_dims = 2\n",
    "val LmarkMat_pcomps = cLmarkDist * eigVecs\n",
    "val LmarkMat_inv = pinv(LmarkMat_pcomps)*(-0.5)\n",
    "val mean_vec = mean(LmarkMat(::,*)) \n",
    "val mean_vec_bc = sc.broadcast(mean_vec)\n",
    "val LmarkMat_inv_bc = sc.broadcast(LmarkMat_inv)\n",
    "\n",
    "//check this carefully whether you need to take column or row mean, breeze\n",
    "// stores matrix in column based format unlike mnumpy which stores it in row based format\n",
    "\n",
    "val convert = funcs.udf((array : Seq[Double]) => {mlVs.dense(array.toArray)})\n",
    "val NLmark_dist_diff = nlmark_dist.withColumn(\"dist\", convert(funcs.col(\"dist\")))\n",
    "\n",
    "def create_embedding(x:mlV):mlV={\n",
    "\n",
    "    val diff = new BDM(num_landmarks, 1, (BDV( x.toArray ) - mean_vec_bc.value.t).toArray )\n",
    "    val embedding = LmarkMat_inv_bc.value * diff\n",
    "    return(mlVs.dense(embedding.toArray))\n",
    "}\n",
    "val udf_embed = funcs.udf((x:mlV) => create_embedding(x) )\n",
    "\n",
    "val NL_isomap = NLmark_dist_diff.withColumn(\"diff\", udf_embed(funcs.col(\"dist\")))\n",
    "\n",
    "// combine landmark and non-landmark embeddings\n",
    "\n",
    "val lmark_embed_arr = LmarkMat_pcomps.t.toArray\n",
    "\n",
    "var temp1:Array[Tuple2[Long, mlV]] = Array()\n",
    "\n",
    "val sorted_ids:Seq[Int] = landmark_ids.toSeq.sortWith(_<_)\n",
    "\n",
    "for(i <- (1 to num_landmarks)){\n",
    "\n",
    "    temp1 = temp1 ++ Array((sorted_ids(i-1).toLong,mlVs.dense(lmark_embed_arr.slice((i-1)*n_dim, i*n_dim))))\n",
    "\n",
    "}\n",
    "val lmark_embed_df = spark.createDataFrame( sc.parallelize(temp1)).toDF(\"id\", \"diff\") \n",
    "\n",
    "\n",
    "val L_isomap_embed =  lmark_embed_df.union(NL_isomap.drop(\"dist\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the embedded dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pic](Lisomap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros ans Cons of using Isomaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isomap is more powerful than other dimensionality reduction algorithms. It works well on non-linear manifolds \n",
    "and gives a closed form solution. But it performs poorly when manifols is not well sampled and contains  holes.\n",
    "Also, as mentioned earlier neighborhood graph creation is tricky and slightly wrong parameters can lead to bad\n",
    "results. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
