{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### MDS - Multi-Dimensional Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non Linear dimensionality reduction methods assume that the latent variables are given by a non linear mapping of observed variables. Non linear methods of dimensionality reduction are very popular. They are much more powerful than linear ones because the relationship between latent and observed variables may be more richer than a simple matrix multiplication. These mappings are function of large number of variables and require large dataset samples and computing power. For example MDS(multidimensional scaling), ISOMAP(isometric mapping), LLE(locally linear embedding) e.t.c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDS means multidimensional scaling. It is not a single method but a family of methods. MDS takes a dissimilarity matrix $D$ where $D_{ij}$ represents the dissimialrity between points $i$ and $j$ and produces a mapping on lower dimension preserving the disimilarities as closely as possible. The disimilairty matrix could be observed or calculated from the given dataset.  MDS has been widely popular and developed in field of human sciences like sociology, anthropology and especially in pshycometrics.  \n",
    "Let's understand it better with an example.   Table below represents correlations  between rates of different types of crimes from US in 1970. Even for a small number of variables we are unable to understand the structure of the correlation.\n",
    "$\\begin{array}{r|lcr}\n",
    "\\text{crime} & \\text{Murder} & \\text{Rape} & \\text{Robbery} & \\text{Assault} & \\text{Burglary} & \\text{Larceny} & \\text{Auto Theft}\\\\\n",
    "\\hline\n",
    "\\text{Murder} & 1.00 & 0.52 & 0.34 & 0.81 & 0.28 & 0.06 & 0.11\\\\\n",
    "\\text{Rape} & 0.52 & 1.00 & 0.55 & 0.70 & 0.68 & 0.60 & 0.44\\\\\n",
    "\\text{Robbery} & 0.34 & 0.55 & 1.00 & 0.56 & 0.62 & 0.44 & 0.62\\\\\n",
    "\\text{Assault} & 0.81 & 0.70 & 0.56 & 1.00 & 0.52 & 0.32 & 0.33\\\\\n",
    "\\text{Burglary} & 0.28 & 0.68 & 0.62 & 0.52 & 1.00 & 0.80 & 0.70\\\\\n",
    "\\text{Larceny} & 0.06 & 0.60 & 0.44 & 0.32 & 0.80 & 1.00 & 0.55\\\\\n",
    "\\text{Auto Theft} & 0.11 & 0.44 & 0.62 & 0.33 & 0.70 & 0.55 & 1.00\\\\\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "Plot below represents the 2-D mappings created from the MDS.\n",
    "![alt text](mds.png \"\")\n",
    "\n",
    "\n",
    "The relative position of points on the plot depends on the disimilairality between them in the table  of correlations, i.e. crime rates which share high correlation are mapped close to each other while crime rates which do not share high correlation are mapped far. From the figure we can see that along horizontal dimension crime distribution can be interpeted as \"violent crime vs property crime\" whereas on vertical dimension distribution of crime can be interpeted as the \"street crime vs hidden crime\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "MDS can be divided into categories-  \n",
    "**Metric MDS** - Metric MDS is used for the quantitative data and tries to preserve the original dissimilarity metrics.  Given a dissimialrity matrix $D$, a monotone function f, and p(number of dimension in subspace) metric MDS tries to find an optimal configuration $X \\subset R^p\\; s.t.\\;\\;\\;$ $f(D_{ij})\\approx d_{ij}=(x_i - x_j)^2$ . Another version of metric MDS is classical MDS(original MDS) formulation which provides closed form solution. Intead of trying to approximate the dissimilarity metrics in lower dimension, it uses eigenvalue decomposition for solution.    \n",
    "**Non-Metric MDS**  - Non-metric MDS is used for ordinal data. It tries to keep the order of dissimialrity metrics intact. For example if $P_{ij}$ is dissimilarity between $i_{th}$ & $j_{th}$ and $P_{32}$ > $P_{24}$, then non-metric mds creates a mapping $s.t. \\;\\;d_{32} >  d_{24}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement metric MDS using SMACOF( scaling by majorization of complicated function) algorithm. Before diving into implementation of metric MDS, we need to learn a bit about MM( Majorization- Minorization) algortihm for optimization.  \n",
    "**MM for finding an optima of a function**  \n",
    "MM algorith is an iterative algorithm for finding optimum of a complex function.\n",
    "Suppose we have a function $f(x)$ for which we need to find a minimum. Instead of directly optimizing $f(x)$ MM uses an approximating function $g(x,x_m)$ to find an optimum. If problem is to find a minimum of $f(x)$ then, $g(x,x_m)$ is called majorizing function else minorizing function and $x_m$ is called support point.  \n",
    "If $g(x,x_m)$ is majorizing function of $f(x)$ then, it has to satisfy following conditions  \n",
    " 1. Optimizing $g(x,x_m)$ should be easier than $f(x)$.\n",
    " 2. For any $x$, $\\;f(x) \\; \\le \\;g(x,x_m)$\n",
    " 3. $f(x_m) = g(x_m,x_m)$\n",
    "\n",
    "Steps of MM algorithm \n",
    "1. choose a random support point $x_m$\n",
    "2. find $x_{min}$ = $\\arg\\min_x {g(x,x_m)}$\n",
    "3. if $f(x_{min}) - f(x_m) \\approx \\epsilon$ where $\\epsilon$ is a very small number else go to step 4\n",
    "4. set $x_m = x_{min}$ and go to step 2  \n",
    "We will understand these steps better with the help of widget below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f54eac9fb8d4e259c531c6fb358d902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='t', max=12, min=1), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import optimize as opt\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "x = np.array(np.arange(100))/10\n",
    "y = np.abs(x-1)+np.abs(x-4)+np.abs(x-3)+np.abs(x-7)+np.abs(x-8)\n",
    "pivot = 9\n",
    "\n",
    "g_vals=[]\n",
    "ax = []\n",
    "ay = []\n",
    "ax_ = 0\n",
    "ay_ = 0\n",
    "num_iter = 0\n",
    "minima = 0\n",
    "fun_minima = 0\n",
    "g_x=0\n",
    "plt.rcParams[\"figure.figsize\"] = [12,6]\n",
    "colors = cm.Reds(range(500))\n",
    "\n",
    "def g(x):\n",
    "    \n",
    "    g1_ = (((x-1)**2)/(np.abs(pivot-1)) + ((x-4)**2)/(np.abs(pivot-4)) +((x-3)**2)/(np.abs(pivot-3)) + \n",
    "              ((x-7)**2)/(np.abs(pivot-7)) + ((x-8)**2)/(np.abs(pivot-8))  )\n",
    "    \n",
    "    g2_ = np.abs(pivot-1) + np.abs(pivot-4) + np.abs(pivot-3) + np.abs(pivot-7) + np.abs(pivot-8)\n",
    "    \n",
    "    g_ = 0.5*(g1_ + g2_)\n",
    "    return (g_)\n",
    "\n",
    "def calc(j):\n",
    "    \n",
    "    global pivot, ax, ay, g_vals,ax_, minima, fun_min, ay_,g_x\n",
    "    for i in range(j):\n",
    "        min_ = opt.minimize(g,x0=0)\n",
    "        minima = min_.x[0]\n",
    "        fun_min = min_.fun\n",
    "        g_x = g(x)\n",
    "        ay_ =  g(pivot)\n",
    "        g_vals = g_vals + [g_x]\n",
    "        ax_ = pivot\n",
    "        pivot = minima\n",
    "        ax = ax + [ax_]\n",
    "        ay = ay + [ay_]\n",
    "    \n",
    "       \n",
    "    \n",
    "def plotter():\n",
    "    \n",
    "    plt.plot(x,y,\"g\")\n",
    "    plt.ylim(10,25)\n",
    "    \n",
    "    \n",
    "    z = int(500/((len(g_vals)+1)))\n",
    "    cmap = colors[0::z]\n",
    "    \n",
    "    l_width = 1\n",
    "    for gx,c,x_pivot,y_pivot in zip(g_vals,cmap[0:-1], ax, ay):\n",
    "        \n",
    "        \n",
    "        plt.plot(x,gx,c=c, lw =l_width, ls=\"dashed\")\n",
    "        plt.plot(x_pivot, y_pivot,\"ro\")\n",
    "        plt.plot([x_pivot,x_pivot],[y_pivot,0])\n",
    "\n",
    "    plt.plot(x, g_x,  c=cmap[-1], lw=l_width+1)\n",
    "    txt1 = \"pivot=(\"+str(\"{0:.2f}\".format(ax_))+\",\"+str(\"{0:.2f}\".format(ay_))+\")\"\n",
    "    plt.annotate(txt1, xy= (ax_, ay_), xycoords=\"data\", xytext=[ax_-2, ay_+2],\n",
    "                    arrowprops=dict(arrowstyle=\"->\",connectionstyle=\"arc3\"),\n",
    "                    bbox=dict(boxstyle=\"round\", fc=\"w\"), family=\"monospace\",\n",
    "                            style=\"normal\", size=\"xx-large\")\n",
    "    \n",
    "    txt1 = \"minima=(\"+str(\"{0:.2f}\".format(minima))+\",\"+str(\"{0:.2f}\".format(fun_min))+\")\"\n",
    "    plt.annotate(txt1, xy= (minima, fun_min), xycoords=\"data\", xytext=[minima-5, fun_min+2],\n",
    "                    arrowprops=dict(arrowstyle=\"->\",\n",
    "                            connectionstyle=\"arc3\"),bbox=dict(boxstyle=\"round\", fc=\"w\"), family=\"monospace\",\n",
    "                            style=\"normal\",size=\"xx-large\")\n",
    "    \n",
    "    \n",
    "    plt.plot(ax_, ax_,\"ro\")\n",
    "    plt.plot(minima, fun_min,\"go\")\n",
    "    plt.plot([ax_,ax_],[ay_,0])\n",
    "    plt.plot([minima, minima],[fun_min,0])\n",
    "    plt.text(0,24,\"Iterations = \"+str(num_iter+1), horizontalalignment=\"left\", verticalalignment=\"top\",\n",
    "            size=\"xx-large\", family=\"monospace\", bbox=dict(boxstyle=\"round\", fc=\"w\"))\n",
    "    \n",
    "    \n",
    "def plot(t):\n",
    "    global num_iter\n",
    "    global num_iter\n",
    "    if t>num_iter:\n",
    "        calc(t-num_iter)\n",
    "        plotter()\n",
    "        num_iter=t\n",
    "    elif t<num_iter:\n",
    "        reset()\n",
    "        calc(t)\n",
    "        plotter()\n",
    "        num_iter = t\n",
    "    \n",
    "    \n",
    "def reset():\n",
    "    global pivot, g_vals, ax, ay, ax_, ay_, minima, fun_min, num_iter \n",
    "    pivot = 9\n",
    "    g_vals=[]\n",
    "    ax = []\n",
    "    ay = []\n",
    "    ax_ = 0\n",
    "    ay_ = 0\n",
    "    g_x = 0\n",
    "    num_iter = 0\n",
    "    minima = 0\n",
    "    fun_min = 0\n",
    "\n",
    "    \n",
    "interact(plot,t = widgets.IntSlider(min=1, max=12, value=1, step=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot in green is the function for which we need to find minimum. Number of iterations **t** is controlled by a widget at the top. Our initial pivot point is $x_m = 9.00$, the minimum of $g(x,9)$ is at $x_{min} = 6.49$. Now if we move the nob of widget to 2, we see that $x_{m}$ is now 6.49 and we have a new $x_{min}\\;=6.20\\;$ based on $g(x,6.49)$.If we increase the **t** by 1 again $x_{m}$ becomes 6.20 and $x_{min}$ value changes to 5.84. As we move the nob further towards right, we see that minima moves towards the minimum of green plot. \n",
    "So, that's how MM algorithm works. The most important part of MM algorithm is to find a good approximating function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now , let's move to SMACOF algorithm for metric MDS. As it was said earlier that metric MDS tries to appoximate the \n",
    "dissimilarity matrix and minimizes the stress function which is given by  \n",
    "\n",
    "$\\hspace{7em}\\sigma(X) = \\Sigma_{i<j}\\; w_{ij}(\\delta_{ij} - d_{ij}(X))^2$ , where  \n",
    "$\\hspace{7em} w_{ij}$ is weightage assigned to disimilarity between $i$ and $j$  \n",
    "$\\hspace{7em} \\delta_{ij}$ is element of the given dissimilarity matrix  \n",
    "$\\hspace{7em} d_{ij}(X)$ is the dissimilarity from the $X$ which we need to find  \n",
    "We aren't going to delve into derivation of the majorizing function for stress function. If you wish to follow please consult excellent [book](http://www.springer.com/in/book/9780387251509) (topic - Majorizing stress, page -187).  \n",
    "The majorizing function for the stress function is given by $\\tau(X,Z)$  \n",
    "$\\hspace{7em} \\tau(X,Z) = \\eta_{\\delta}^2 + tr \\;X^TVX + 2tr\\; X^TB(Z)Z$  , where     \n",
    "$\\hspace{9em} \\eta_{\\delta}^2 = \\Sigma_{i<j} W_{ij}\\delta_{ij}^2$  \n",
    "$\\hspace{9em} B(Z)$ contains elements  \n",
    "$\\hspace{11em} b_{ij} = w_{ij}*\\delta_{ij}/d_{ij} \\;\\; if \\;d_{ij} > \\;\\text{0.0} \\; else\\; \\text{0.0}$  \n",
    "$\\hspace{11em}  b_{ii} = - \\Sigma_{j,j\\ne i}\\; b_{ij}$  \n",
    "$\\hspace{9em}  V = \\Sigma_{i<j} w_{ij}*A_{ij}$  \n",
    "$\\hspace{11em}  A$ is a sparse matrix same size as $W$ and $s.t. A_{ii}=A_{jj}=1, A_{ji}=A_{ij}=-1$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the SMACOF algorithm, we need the derivative of $t(X,Z)$ $w.t.r$ $X$.  \n",
    "$\\hspace{7em} \\nabla\\;\\tau(X,Z) = 2  XV + 2  B(Z)Z \\; = \\; 0$  which gives  \n",
    "$\\hspace{7em}VX = B(Z)Z$.  \n",
    "$\\hspace{7em}X = V^+B(Z)Z\\;$ where $V^+$ is psuedo-inverse of $V$\n",
    "\n",
    "\n",
    "Now, we have everything for the steps of SMACOF algorithm.\n",
    "1. choose a random point $X_m$\n",
    "2. Find the minimum of $\\tau(X, X_m)$ which is given by $X_{min} = X = V^+B(Z)Z$\n",
    "3. if $\\sigma(X_{m}) - \\sigma(X_{min}) \\lt \\epsilon$ break\n",
    "      else set $X_m = X_{min}$ and go to step 2\n",
    "      \n",
    "If we don't  differentiate between dissimilarities $d_{ij}$ and $d_{jk}$, then $w_{ij}$ = 1.\n",
    "Then, $V^+ = n^{-1}J$ where J is the centering matrix. Our update equation changes to  \n",
    "$X_{min} = n^{-1}B(Z)(Z)$.  \n",
    "\n",
    "Let's code the above steps of SMACOF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "from sklearn import datasets\n",
    "\n",
    "iris_d = datasets.load_iris()\n",
    "iris = iris_d.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the variables which require calculation only once\n",
    "from sklearn.metrics import pairwise\n",
    "\n",
    "# distance metric calulation function\n",
    "dist_mat = lambda x:pairwise.pairwise_distances(x, metric = \"euclidean\")\n",
    "\n",
    "# we will denote the original distance metric with D\n",
    "D = dist_mat(iris)\n",
    "\n",
    "# stress calculation function\n",
    "stress = lambda d: ((D-d)**2).sum()\n",
    "\n",
    "def create_B(d):\n",
    "    \n",
    "    d[d==0.0] = np.inf\n",
    "    B = D/d\n",
    "    np.fill_diagonal(B, 0.0)\n",
    "    \n",
    "    B[range(D.shape[0]),range(D.shape[0])] = -B.sum(axis=0).T\n",
    "    return(B)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# steps of SMACOF\n",
    "np.random.seed(101)\n",
    "max_iter = 1000\n",
    "\n",
    "# set lower dimenion value to 2 \n",
    "p = 2\n",
    "n,m = iris.shape\n",
    "#choose a random pivot point\n",
    "x_m = np.random.rand(n, p)\n",
    "\n",
    "# denote the subspace distance matrix as d\n",
    "d = dist_mat(x_m)\n",
    "\n",
    "stress_old = stress(d)\n",
    "\n",
    "tol = 1e-4\n",
    "for i in range(1000):\n",
    "    print(i)\n",
    "    x_min = create_B(d.copy()).dot(x_m)/n\n",
    "    \n",
    "    d = dist_mat(x_min)\n",
    "    stress_new = stress( d)\n",
    "    if stress_old-stress_new < tol:\n",
    "        break\n",
    "    else:\n",
    "        x_m = x_min\n",
    "        stress_old = stress_new\n",
    "    \n",
    "\n",
    "\n",
    "plt.plot(x_min[:,0],x_min[:,1],\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only hyperparameter required is number of dimensions in lower dimensions. If  number of dimensions is low, then solution will get distorted due to over-compression. If It is high then over-fitting will happen and solution will fit the random noise. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS in pyspark\n",
    "Let's convert the python code to pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import math as ma\n",
    "import numpy as np\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "\n",
    "\n",
    "iris_ = datasets.load_iris()\n",
    "\n",
    "iris = iris_.data\n",
    "# repartitoning the dataframe by id column will speed up the join operation \n",
    "\n",
    "df = spark.createDataFrame(sc.parallelize(iris.tolist()).zipWithIndex()).toDF(\"features\", \"id\").repartition(\"id\")\n",
    "df.cache()\n",
    "\n",
    "euclidean = lambda x,y:ma.sqrt(np.sum((np.array(x)-np.array(y))**2))\n",
    "data_bc = sc.broadcast(df.sort(\"id\").select(\"features\").rdd.collect())\n",
    "\n",
    "# create the distance metric\n",
    "def pairwise_metric1(y):\n",
    "    dist = []\n",
    "    for x in data_bc.value:\n",
    "        dist += [ma.sqrt(np.sum((np.array(x)-np.array(y))**2))]\n",
    "    \n",
    "    return(dist)\n",
    "\n",
    "udf_dist1 = f.udf(pairwise_metric1, t.ArrayType(t.DoubleType()))\n",
    "\n",
    "df = df.withColumn(\"D\", udf_dist1(\"features\"))\n",
    "n,p = iris.shape\n",
    "dim = 2\n",
    "X = np.random.rand(n,dim)\n",
    "\n",
    "# randomly initialize a solution for the pivot point.\n",
    "dfrand = spark.createDataFrame(sc.parallelize(X.tolist()).zipWithIndex()).toDF(\"X\", \"id2\").repartition(\"id2\")\n",
    "df = df.join(dfrand, df.id==dfrand.id2, \"inner\").drop(\"id1\")\n",
    "\n",
    "def pairwise_metric2(y):\n",
    "    dist = []\n",
    "    for x in X_bc.value:\n",
    "        dist += [ma.sqrt(np.sum((np.array(x)-np.array(y))**2))]\n",
    "    return(dist)\n",
    "\n",
    "# create the matrix B\n",
    "def B(id,x,y):\n",
    "    \n",
    "    y,x = np.array(y), np.array(x) \n",
    "    y[y==0.0] = np.inf\n",
    "    z = -x/y\n",
    "    \n",
    "    z[id] = -(np.sum(z)-z[id])\n",
    "    return(z.tolist())\n",
    "\n",
    "# function for matrix multiplication using outer multiplication\n",
    "def df_mult(df, col1, col2, n1, n2, matrix=True):\n",
    "    \n",
    "    udf_mult = f.udf(lambda x,y:np.outer(np.array(x), np.array(y)).flatten().tolist(), t.ArrayType(t.DoubleType()))\n",
    "    \n",
    "    df = df.withColumn(\"mult\", udf_mult(col1, col2))\n",
    "    df = df.agg(f.array([f.sum(f.col(\"mult\")[i]) for i in range(n1*n2)])).toDF(\"mult\")\n",
    "    if not matrix:\n",
    "        return(df)\n",
    "    st = t.ArrayType(t.StructType([t.StructField(\"id\",t.LongType()),t.StructField(\"row\", t.ArrayType(t.DoubleType()))]))\n",
    "    udf_arange = f.udf(lambda x:[(i,j.tolist()) for i,j in enumerate(np.array(x).reshape(n1,n2)/n1)], st)\n",
    "    \n",
    "    df = df.withColumn(\"mult\", udf_arange(\"mult\")).select(f.explode(\"mult\").alias(\"mult\"))\n",
    "    \n",
    "    df = df.select(f.col(\"mult.id\").alias(\"id2\"),f.col(\"mult.row\").alias(\"X_min\")).repartition(\"id2\")\n",
    "    return(df)\n",
    "\n",
    "udf_B = f.udf(B, t.ArrayType(t.DoubleType()))\n",
    "udf_sigma = f.udf(lambda x,y: float(np.sum((np.array(x)-np.array(y))**2)), t.DoubleType())\n",
    "sigma_old = np.inf\n",
    "tol = 1e-4\n",
    "max_iter = 1000\n",
    "for i in range(max_iter):\n",
    "    X_bc = sc.broadcast(df.sort(\"id\").select(\"X\").rdd.collect())\n",
    "    def pairwise_metric2(y):\n",
    "        dist = []\n",
    "        for x in X_bc.value:\n",
    "            dist += [ma.sqrt(np.sum((np.array(x)-np.array(y))**2))]\n",
    "        return(dist)\n",
    "    udf_dist2 = f.udf(pairwise_metric2, t.ArrayType(t.DoubleType()))\n",
    "    df = df.withColumn(\"di\", udf_dist2(\"X\"))\n",
    "    \n",
    "    df = df.withColumn(\"sigma\", udf_sigma(\"D\",\"di\"))\n",
    "    sigma_new = df.agg({\"sigma\":\"sum\"}).collect()[0][0]\n",
    "    print(sigma_old, sigma_new)\n",
    "    sigma_old = sigma_new\n",
    "    df = df.withColumn(\"B\", udf_B(\"id\",\"D\",\"di\")).drop(\"di\")\n",
    "    \n",
    "    X_min = df_mult(df, \"B\", \"X\", n, dim)\n",
    "    \n",
    "    df = df.join(X_min, df.id==X_min.id2).select(\"id\", \"D\", f.col(\"X_min\").alias(\"X\"))\n",
    "    # cache action will prevent recreation of dataframe from base\n",
    "    df.cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of **MNIST** dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages and drawbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric MDs algorithm works better than classical MDS on non-linear manifolds. But, it requires $O(N^2)$ operations for processing the distance matrix. Embedding new data is hard in metric MDS. Iterative algorithms are require a lot of computing power in spark. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
